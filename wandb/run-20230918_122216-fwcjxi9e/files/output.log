2023-09-18 12:22:20,101 - root - INFO - Using device: cuda
2023-09-18 12:22:20,123 - root - INFO - Quadro M5000
2023-09-18 12:22:20,123 - root - INFO - Memory Usage:
2023-09-18 12:22:20,124 - root - INFO - Allocated: 0.0 GB
2023-09-18 12:22:20,124 - root - INFO - Cached: 0.0 GB
2023-09-18 12:22:20,279 - root - ERROR - Error reading validation dataset.
2023-09-18 12:22:20,279 - root - INFO - Preprocessing Internal_301_DB_layer3output dataset for train split.











100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63210/63210 [00:24<00:00, 2567.01it/s]
Augmenting dataset: 63210it [00:00, 2156602.72it/s]
2023-09-18 12:23:09,769 - root - INFO - Preprocessing Internal_301_DB_layer3output dataset for test split.
  0%|                                                                                                                                                          | 0/6622 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/a-150/work/finger-vein-quality-assessement/train.py", line 131, in <module>
    main()
  File "/home/a-150/work/finger-vein-quality-assessement/train.py", line 118, in main
    train(
  File "/home/a-150/work/finger-vein-quality-assessement/common/train_pipeline/train.py", line 158, in train
    train_dataset, validation_dataset, _ = get_dataset(environment, batch_size)
                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/a-150/work/finger-vein-quality-assessement/common/train_pipeline/train.py", line 54, in get_dataset
    return datasets.get_dataset(environment, batch_size=batch_size)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/a-150/work/finger-vein-quality-assessement/common/util/data_pipeline/dataset_chainer.py", line 67, in get_dataset
    self._compile_all_datasets()
  File "/home/a-150/work/finger-vein-quality-assessement/common/util/data_pipeline/dataset_chainer.py", line 31, in _compile_all_datasets
    sets = dataset.compile_sets()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/a-150/work/finger-vein-quality-assessement/common/util/data_pipeline/dataset_loader.py", line 95, in compile_sets
    converted_dataset = self._convert_to_numpy_dataset(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/a-150/work/finger-vein-quality-assessement/common/util/data_pipeline/dataset_loader.py", line 61, in _convert_to_numpy_dataset
    pre_processed_data.append(self.pre_process(data))
                              ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/a-150/work/finger-vein-quality-assessement/common/data_pipeline/dataset.py", line 111, in pre_process
    image = np.load(data.path)
            ^^^^^^^^^^^^^^^^^^
  File "/home/a-150/anaconda3/envs/dl-torch/lib/python3.11/site-packages/numpy/lib/npyio.py", line 462, in load
    raise ValueError("Cannot load file containing pickled data "
ValueError: Cannot load file containing pickled data when allow_pickle=False