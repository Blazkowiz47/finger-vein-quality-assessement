2023-09-18 12:24:56,427 - root - INFO - Using device: cuda
2023-09-18 12:24:56,447 - root - INFO - Quadro M5000
2023-09-18 12:24:56,447 - root - INFO - Memory Usage:
2023-09-18 12:24:56,447 - root - INFO - Allocated: 0.0 GB
2023-09-18 12:24:56,448 - root - INFO - Cached: 0.0 GB
2023-09-18 12:24:56,599 - root - ERROR - Error reading validation dataset.
2023-09-18 12:24:56,599 - root - INFO - Preprocessing Internal_301_DB_layer3output dataset for train split.





































































































































































































































































































































































































































100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63210/63210 [14:05<00:00, 74.72it/s]
Augmenting dataset: 63210it [00:00, 1292861.59it/s]
2023-09-18 12:39:34,937 - root - INFO - Preprocessing Internal_301_DB_layer3output dataset for test split.
  0%|                                                                                                                                                          | 0/6622 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/a-150/anaconda3/envs/dl-torch/lib/python3.11/site-packages/numpy/lib/npyio.py", line 465, in load
    return pickle.load(fid, **pickle_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_pickle.UnpicklingError: pickle data was truncated
The above exception was the direct cause of the following exception:
Traceback (most recent call last):
  File "/home/a-150/work/finger-vein-quality-assessement/train.py", line 131, in <module>
  File "/home/a-150/work/finger-vein-quality-assessement/train.py", line 118, in main
    )
  File "/home/a-150/work/finger-vein-quality-assessement/common/train_pipeline/train.py", line 158, in train
    train_dataset, validation_dataset, _ = get_dataset(environment, batch_size)
                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/a-150/work/finger-vein-quality-assessement/common/train_pipeline/train.py", line 54, in get_dataset
    return datasets.get_dataset(environment, batch_size=batch_size)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/a-150/work/finger-vein-quality-assessement/common/util/data_pipeline/dataset_chainer.py", line 67, in get_dataset
    self._compile_all_datasets()
  File "/home/a-150/work/finger-vein-quality-assessement/common/util/data_pipeline/dataset_chainer.py", line 31, in _compile_all_datasets
    sets = dataset.compile_sets()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/a-150/work/finger-vein-quality-assessement/common/util/data_pipeline/dataset_loader.py", line 95, in compile_sets
    converted_dataset = self._convert_to_numpy_dataset(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/a-150/work/finger-vein-quality-assessement/common/util/data_pipeline/dataset_loader.py", line 61, in _convert_to_numpy_dataset
    pre_processed_data.append(self.pre_process(data))
                              ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/a-150/work/finger-vein-quality-assessement/common/data_pipeline/dataset.py", line 111, in pre_process
    image = np.load(data.path, allow_pickle=True)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/a-150/anaconda3/envs/dl-torch/lib/python3.11/site-packages/numpy/lib/npyio.py", line 467, in load
    raise pickle.UnpicklingError(
_pickle.UnpicklingError: Failed to interpret file 'datasets/layer3output/test/129/86.bmp' as a pickle